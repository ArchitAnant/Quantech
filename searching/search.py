# Import necessary modules and classes
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.embeddings import OllamaEmbeddings  # Ensure this import is correct

# Function to load and split the content of a PDF file
def load_and_split_pdf(file_path):
    # Load the PDF file
    loader = PyPDFLoader(file_path)
    # Split the content of the PDF into chunks
    data = loader.load()  
    return data

# Function to set up the vector database for storing document chunks and embeddings
def setup_vector_db(data, model="nomic-embed-text", chunk_size=7500, chunk_overlap=100):
    # Split the document into chunks using a recursive character text splitter
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = text_splitter.split_documents(data)
    
    # Create a vector database from the document chunks with embeddings generated by the specified model
    vector_db = Chroma.from_documents(
        documents=chunks,
        embedding=OllamaEmbeddings(model=model, show_progress=True),  
        collection_name="local-rag"  
    )
    return vector_db

# Function to set up the retriever for querying the vector database
def setup_retriever(vector_db, llm_model="llama2"):
    # Define a prompt template to generate alternative versions of the user's question
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""You are an AI language model assistant. Your task is to generate a answer of 
        a question from pdf.
        Original question: {question}""",
    )
    
    # Create an instance of ChatOllama using the specified LLM model
    llm = ChatOllama(model=llm_model)
    
    # Set up the retriever that will query the vector database using multiple variations of the question
    retriever = MultiQueryRetriever.from_llm(
        vector_db.as_retriever(),
        llm,
        prompt=QUERY_PROMPT
    )
    return retriever

# Function to run a RAG (Retrieval-Augmented Generation) query and get an answer
def run_rag_query(retriever, question):
    # Define the prompt template that will be used to generate the answer from the context
    template = """Answer the question based ONLY on the following context:
    {context}
    Question: {question}
    """
    
    # Create a ChatPromptTemplate from the template string
    prompt = ChatPromptTemplate.from_template(template)
    
    # Define the chain of operations for the query: retrieve context, pass the question, generate an answer, and parse it
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}  # Pass the question through unchanged
        | prompt  # Generate a prompt using the retrieved context
        | llm  # Use the LLM to generate an answer
        | StrOutputParser()  # Parse the LLM's output into a string
    )
    return chain.invoke(question)  # Execute the chain with the given question

# Function to allow the user to interactively provide a file path and ask questions
def interactive_query():
    # Prompt the user to input the path to the PDF file
    file_path = input("Please enter the PDF file path: ")
    
    # Load and split the PDF content
    data = load_and_split_pdf(file_path)
    
    # Set up the vector database with the split data
    vector_db = setup_vector_db(data)
    
    # Set up the retriever for querying the vector database
    retriever = setup_retriever(vector_db)

    # Loop to allow the user to ask multiple questions
    while True:
        # Prompt the user to input a question
        question = input("Ask your question (or type 'exit' to quit): ")
        
        # Check if the user wants to exit the interactive session
        if question.lower() == 'exit':
            print("Exiting the interactive session.")
            break
        
        # Run the RAG query with the user's question and print the response
        response = run_rag_query(retriever, question)
        print(f"Response: {response}")

# Example usage: start the interactive query session
if __name__ == "__main__":
    interactive_query()
